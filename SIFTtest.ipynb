{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "#import pdb  \n",
    "#pdb.set_trace()#turn on the pdb prompt  \n",
    "  \n",
    "#read image  \n",
    "img = cv2.imread('7elevenLogo.jpg',cv2.IMREAD_COLOR)  \n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \n",
    "cv2.threshold(gray,127,255,cv2.THRESH_BINARY)\n",
    "cv2.imshow('origin',img); \n",
    "  \n",
    "#SIFT  \n",
    "detector = cv2.SIFT()  \n",
    "keypoints = detector.detect(gray,None)  \n",
    "img = cv2.drawKeypoints(gray,keypoints)  \n",
    "# img = cv2.drawKeypoints(gray,keypoints,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)  \n",
    "# print type(keypoints[0])\n",
    "cv2.imshow('test',img);  \n",
    "cv2.waitKey(0)  \n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read image  \n",
    "img = cv2.imread('simitsu2.jpg',cv2.IMREAD_COLOR)  \n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \n",
    "cv2.threshold(gray,127,255,cv2.THRESH_BINARY)\n",
    "cv2.imshow('origin',img); \n",
    "  \n",
    "#SIFT  \n",
    "detector = cv2.SIFT()  \n",
    "keypoints = detector.detect(gray,None)  \n",
    "# img = cv2.drawKeypoints(gray,keypoints)  \n",
    "img = cv2.drawKeypoints(gray,keypoints,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)  \n",
    "cv2.imshow('test',img);  \n",
    "cv2.waitKey(0)  \n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畫出match的畫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def drawMatches(img1, kp1, img2, kp2, matches):\n",
    "    \"\"\"\n",
    "    My own implementation of cv2.drawMatches as OpenCV 2.4.9\n",
    "    does not have this function available but it's supported in\n",
    "    OpenCV 3.0.0\n",
    "\n",
    "    This function takes in two images with their associated \n",
    "    keypoints, as well as a list of DMatch data structure (matches) \n",
    "    that contains which keypoints matched in which images.\n",
    "\n",
    "    An image will be produced where a montage is shown with\n",
    "    the first image followed by the second image beside it.\n",
    "\n",
    "    Keypoints are delineated with circles, while lines are connected\n",
    "    between matching keypoints.\n",
    "\n",
    "    img1,img2 - Grayscale images\n",
    "    kp1,kp2 - Detected list of keypoints through any of the OpenCV keypoint \n",
    "              detection algorithms\n",
    "    matches - A list of matches of corresponding keypoints through any\n",
    "              OpenCV keypoint matching algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new output image that concatenates the two images together\n",
    "    # (a.k.a) a montage\n",
    "    rows1 = img1.shape[0]\n",
    "    cols1 = img1.shape[1]\n",
    "    rows2 = img2.shape[0]\n",
    "    cols2 = img2.shape[1]\n",
    "\n",
    "    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')\n",
    "\n",
    "    # Place the first image to the left\n",
    "    out[:rows1,:cols1] = np.dstack([img1, img1, img1])\n",
    "\n",
    "    # Place the next image to the right of it\n",
    "    out[:rows2,cols1:] = np.dstack([img2, img2, img2])\n",
    "\n",
    "    # For each pair of points we have between both images\n",
    "    # draw circles, then connect a line between them\n",
    "    for mat in matches:\n",
    "\n",
    "        # Get the matching keypoints for each of the images\n",
    "        img1_idx = mat.queryIdx\n",
    "        img2_idx = mat.trainIdx\n",
    "\n",
    "        # x - columns\n",
    "        # y - rows\n",
    "        (x1,y1) = kp1[img1_idx].pt\n",
    "        (x2,y2) = kp2[img2_idx].pt\n",
    "\n",
    "        # Draw a small circle at both co-ordinates\n",
    "        # radius 4\n",
    "        # colour blue\n",
    "        # thickness = 1\n",
    "        cv2.circle(out, (int(x1),int(y1)), 4, (255, 0, 0), 1)   \n",
    "        cv2.circle(out, (int(x2)+cols1,int(y2)), 4, (255, 0, 0), 1)\n",
    "\n",
    "        # Draw a line in between the two points\n",
    "        # thickness = 1\n",
    "        # colour blue\n",
    "        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (255, 0, 0), 1)\n",
    "\n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('Matched Features', out)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyWindow('Matched Features')\n",
    "\n",
    "    # Also return the image if you'd like a copy\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比對兩張圖像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('simitsu1.jpg',0)          # queryImage\n",
    "img2 = cv2.imread('simitsu2.jpg',0) # trainImage\n",
    "\n",
    "# Initiate SIFT detector\n",
    "orb = cv2.ORB()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-0bdf3c22a32f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Draw first 10 matches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mimg3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrawMatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-9bb8018e3f3f>\u001b[0m in \u001b[0;36mdrawMatches\u001b[1;34m(img1, kp1, img2, kp2, matches)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Create a new output image that concatenates the two images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# (a.k.a) a montage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mrows1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mcols1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mrows2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 10 matches.\n",
    "img3 = drawMatches(img1,kp1,img2,kp2,matches[:10])\n",
    "\n",
    "plt.imshow(img3),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用sift進行比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches... 191\n",
      "good 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2  \n",
    "import scipy as sp  \n",
    "  \n",
    "img1 = cv2.imread('7elevenLogo.jpg',0) # queryImage  \n",
    "img2 = cv2.imread('7elevenTest.jpg',0) # trainImage  \n",
    "  \n",
    "# Initiate SIFT detector  \n",
    "sift = cv2.SIFT()  \n",
    "  \n",
    "# find the keypoints and descriptors with SIFT  \n",
    "kp1, des1 = sift.detectAndCompute(img1,None)  \n",
    "kp2, des2 = sift.detectAndCompute(img2,None)  \n",
    "  \n",
    "# FLANN parameters  \n",
    "FLANN_INDEX_KDTREE = 0  \n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)  \n",
    "search_params = dict(checks=50)   # or pass empty dictionary  \n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)  \n",
    "matches = flann.knnMatch(des1,des2,k=2)  \n",
    "  \n",
    "print 'matches...',len(matches)  \n",
    "# Apply ratio test  \n",
    "good = []  \n",
    "for m,n in matches:  \n",
    "    if m.distance < 0.75*n.distance:  \n",
    "        good.append(m)  \n",
    "print 'good',len(good)  \n",
    "# #####################################  \n",
    "# visualization  \n",
    "h1, w1 = img1.shape[:2]  \n",
    "h2, w2 = img2.shape[:2]  \n",
    "view = sp.zeros((max(h1, h2), w1 + w2, 3), sp.uint8)  \n",
    "view[:h1, :w1, 0] = img1  \n",
    "view[:h2, w1:, 0] = img2  \n",
    "view[:, :, 1] = view[:, :, 0]  \n",
    "view[:, :, 2] = view[:, :, 0]  \n",
    "  \n",
    "for m in good:  \n",
    "    # draw the keypoints  \n",
    "    # print m.queryIdx, m.trainIdx, m.distance  \n",
    "    color = tuple([sp.random.randint(0, 255) for _ in xrange(3)])  \n",
    "    #print 'kp1,kp2',kp1,kp2  \n",
    "    cv2.line(view, (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1])) , (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1])), color)  \n",
    "  \n",
    "cv2.imshow(\"view\", view)  \n",
    "cv2.waitKey()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用scipy比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches... 191\n",
      "good 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coding=utf-8 \n",
    "import cv2 \n",
    "import scipy as sp \n",
    "  \n",
    "img1 = cv2.imread('7elevenLogo.jpg',0) # queryImage \n",
    "img2 = cv2.imread('7elevenTest.jpg',0) # trainImage \n",
    "  \n",
    "# Initiate SIFT detector \n",
    "sift = cv2.SIFT() \n",
    "  \n",
    "# find the keypoints and descriptors with SIFT \n",
    "kp1, des1 = sift.detectAndCompute(img1,None) \n",
    "kp2, des2 = sift.detectAndCompute(img2,None) \n",
    "  \n",
    "# FLANN parameters \n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5) \n",
    "search_params = dict(checks=50)  # or pass empty dictionary \n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params) \n",
    "matches = flann.knnMatch(des1,des2,k=2) \n",
    "  \n",
    "print 'matches...',len(matches) \n",
    "# Apply ratio test \n",
    "good = [] \n",
    "for m,n in matches: \n",
    "    if m.distance < 0.75*n.distance: \n",
    "        good.append(m) \n",
    "print 'good',len(good) \n",
    "# ##################################### \n",
    "# visualization \n",
    "h1, w1 = img1.shape[:2] \n",
    "h2, w2 = img2.shape[:2] \n",
    "view = sp.zeros((max(h1, h2), w1 + w2, 3), sp.uint8) \n",
    "view[:h1, :w1, 0] = img1 \n",
    "view[:h2, w1:, 0] = img2 \n",
    "view[:, :, 1] = view[:, :, 0] \n",
    "view[:, :, 2] = view[:, :, 0] \n",
    "  \n",
    "for m in good: \n",
    "    #draw the keypoints \n",
    "    # print m.queryIdx, m.trainIdx, m.distance \n",
    "    color = tuple([sp.random.randint(0, 255) for _ in xrange(3)]) \n",
    "    ##print 'kp1,kp2',kp1,kp2 \n",
    "    cv2.line(view, (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1])) , (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1])), color) \n",
    "  \n",
    "cv2.imshow(\"view\", view) \n",
    "cv2.waitKey() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 儲存點特徵準備訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "..\\..\\..\\modules\\imgproc\\src\\color.cpp:3739: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-2af3c12e88f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'car.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: ..\\..\\..\\modules\\imgproc\\src\\color.cpp:3739: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cPickle\n",
    "\n",
    "img = cv2.imread('car.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "sift = cv2.SIFT()\n",
    "kp = sift.detect(gray, None)\n",
    "img = cv2.drawKeypoints(img, keypoints = kp)\n",
    "\n",
    "index = []\n",
    "\n",
    "for point in kp:\n",
    "    temp = (point.pt, point.size, point.angle, point.response, point.octave, \n",
    "            point.class_id) \n",
    "    index.append(temp)\n",
    "\n",
    "## Put the keypoints into a File\n",
    "\n",
    "f = open(\"keypoints.txt\", \"w\")\n",
    "f.write(cPickle.dumps(index))\n",
    "f.close()\n",
    "cv2.imwrite('sift_keypoints.jpg', img)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.0\n",
      "125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0xb843438>, None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('7elevenLogo.jpg',0)\n",
    "# Create SURF object. You can specify params here or later.\n",
    "# Here I set Hessian Threshold to 400\n",
    "surf = cv2.SURF(400)\n",
    "#Find keypoints and descriptors directly\n",
    "kp, des = surf.detectAndCompute(img,None)\n",
    "len(kp)\n",
    "\n",
    "# Check present Hessian threshold\n",
    "print surf.hessianThreshold\n",
    "\n",
    "# We set it to some 50000. Remember, it is just for representing in picture.\n",
    "# In actual cases, it is better to have a value 300-500\n",
    "surf.hessianThreshold = 5000\n",
    "\n",
    "# Again compute keypoints and check its number.\n",
    "kp, des = surf.detectAndCompute(img,None)\n",
    "\n",
    "print len(kp)\n",
    "\n",
    "img2 = cv2.drawKeypoints(img,kp,None,(255,0,0),4)\n",
    "plt.imshow(img2),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0xb37cbe0>, None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check upright flag, if it False, set it to True\n",
    "print surf.upright\n",
    "\n",
    "surf.upright = True\n",
    "\n",
    "# Recompute the feature points and draw it\n",
    "kp = surf.detect(img,None)\n",
    "img2 = cv2.drawKeypoints(img,kp,None,(255,0,0),4)\n",
    "\n",
    "plt.imshow(img2),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "True\n",
      "128\n",
      "(254L, 128L)\n"
     ]
    }
   ],
   "source": [
    "# Find size of descriptor\n",
    "print surf.descriptorSize()\n",
    "\n",
    "# That means flag, \"extended\" is False.\n",
    "print surf.extended\n",
    "\n",
    "# So we make it to True to get 128-dim descriptors.\n",
    "# surf.extended = True\n",
    "kp, des = surf.detectAndCompute(img,None)\n",
    "print surf.descriptorSize()\n",
    "\n",
    "print des.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0xf03cb70>, None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('7elevenLogo.jpg',0)\n",
    "img2 = cv2.imread('7elevenTest.jpg',0)\n",
    "# Create SURF object. You can specify params here or later.\n",
    "# Here I set Hessian Threshold to 400\n",
    "surf = cv2.SURF(400)\n",
    "#Find keypoints and descriptors directly\n",
    "kp1, des1 = surf.detectAndCompute(img1,None)\n",
    "kp2, des2 = surf.detectAndCompute(img2,None)\n",
    "\n",
    "# Check present Hessian threshold\n",
    "print surf.hessianThreshold\n",
    "\n",
    "# We set it to some 50000. Remember, it is just for representing in picture.\n",
    "# In actual cases, it is better to have a value 300-500\n",
    "surf.hessianThreshold = 1000\n",
    "\n",
    "# Again compute keypoints and check its number.\n",
    "kp1, des1 = surf.detectAndCompute(img1,None)\n",
    "kp2, des2 = surf.detectAndCompute(img2,None)\n",
    "\n",
    "\n",
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Match descriptors.\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "# Draw first 10 matches.\n",
    "img3 = drawMatches(img1,kp1,img2,kp2,matches[:10])\n",
    "\n",
    "plt.imshow(img3),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "Uses SURF to match two images.\n",
    "Based on the sample code from opencv:\n",
    "  samples/python2/find_obj.py\n",
    "USAGE\n",
    "  find_obj.py <image1> <image2>\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "\n",
    "###############################################################################\n",
    "# Image Matching\n",
    "###############################################################################\n",
    "\n",
    "def match_images(img1, img2):\n",
    "    \"\"\"Given two images, returns the matches\"\"\"\n",
    "    detector = cv2.SURF(400, 5, 5)\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "    kp1, desc1 = detector.detectAndCompute(img1, None)\n",
    "    kp2, desc2 = detector.detectAndCompute(img2, None)\n",
    "    #print 'img1 - %d features, img2 - %d features' % (len(kp1), len(kp2))\n",
    "\n",
    "    raw_matches = matcher.knnMatch(desc1, trainDescriptors = desc2, k = 2) #2\n",
    "    kp_pairs = filter_matches(kp1, kp2, raw_matches)\n",
    "    return kp_pairs\n",
    "\n",
    "def filter_matches(kp1, kp2, matches, ratio = 0.75):\n",
    "    mkp1, mkp2 = [], []\n",
    "    for m in matches:\n",
    "        if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "            m = m[0]\n",
    "            mkp1.append( kp1[m.queryIdx] )\n",
    "            mkp2.append( kp2[m.trainIdx] )\n",
    "    kp_pairs = zip(mkp1, mkp2)\n",
    "    return kp_pairs\n",
    "    \n",
    "    \n",
    "###############################################################################\n",
    "# Match Diplaying\n",
    "###############################################################################\n",
    "\n",
    "def explore_match(win, img1, img2, kp_pairs, status = None, H = None):\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    vis = np.zeros((max(h1, h2), w1+w2), np.uint8)\n",
    "    vis[:h1, :w1] = img1\n",
    "    vis[:h2, w1:w1+w2] = img2\n",
    "    vis = cv2.cvtColor(vis, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    if H is not None:\n",
    "        corners = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]])\n",
    "        corners = np.int32( cv2.perspectiveTransform(corners.reshape(1, -1, 2), H).reshape(-1, 2) + (w1, 0) )\n",
    "        cv2.polylines(vis, [corners], True, (255, 255, 255))\n",
    "\n",
    "    if status is None:\n",
    "        status = np.ones(len(kp_pairs), np.bool_)\n",
    "    p1 = np.int32([kpp[0].pt for kpp in kp_pairs])\n",
    "    p2 = np.int32([kpp[1].pt for kpp in kp_pairs]) + (w1, 0)\n",
    "\n",
    "    green = (0, 255, 0)\n",
    "    red = (0, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    kp_color = (51, 103, 236)\n",
    "    for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):\n",
    "        if inlier:\n",
    "            col = green\n",
    "            cv2.circle(vis, (x1, y1), 2, col, -1)\n",
    "            cv2.circle(vis, (x2, y2), 2, col, -1)\n",
    "        else:\n",
    "            col = red\n",
    "            r = 2\n",
    "            thickness = 3\n",
    "            cv2.line(vis, (x1-r, y1-r), (x1+r, y1+r), col, thickness)\n",
    "            cv2.line(vis, (x1-r, y1+r), (x1+r, y1-r), col, thickness)\n",
    "            cv2.line(vis, (x2-r, y2-r), (x2+r, y2+r), col, thickness)\n",
    "            cv2.line(vis, (x2-r, y2+r), (x2+r, y2-r), col, thickness)\n",
    "    vis0 = vis.copy()\n",
    "    for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):\n",
    "        if inlier:\n",
    "            cv2.line(vis, (x1, y1), (x2, y2), green)\n",
    "\n",
    "    cv2.imshow(win, vis)\n",
    "\n",
    "\n",
    "  \n",
    "def draw_matches(window_name, kp_pairs, img1, img2):\n",
    "    \"\"\"Draws the matches for \"\"\"\n",
    "    mkp1, mkp2 = zip(*kp_pairs)\n",
    "    \n",
    "    p1 = np.float32([kp.pt for kp in mkp1])\n",
    "    p2 = np.float32([kp.pt for kp in mkp2])\n",
    "    \n",
    "    if len(kp_pairs) >= 4:\n",
    "        H, status = cv2.findHomography(p1, p2, cv2.RANSAC, 5.0)\n",
    "        #print '%d / %d  inliers/matched' % (np.sum(status), len(status))\n",
    "    else:\n",
    "        H, status = None, None\n",
    "        #print '%d matches found, not enough for homography estimation' % len(p1)\n",
    "    \n",
    "    if len(p1):\n",
    "        explore_match(window_name, img1, img2, kp_pairs, status, H)\n",
    "\n",
    "###############################################################################\n",
    "# Test Main\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"Test code: Uses the two specified\"\"\"\n",
    "    if len(sys.argv) < 3:\n",
    "        print \"No filenames specified\"\n",
    "        print \"USAGE: find_obj.py <image1> <image2>\"\n",
    "        sys.exit(1)\n",
    "    \n",
    "#     fn1 = sys.argv[1]\n",
    "#     fn2 = sys.argv[2]\n",
    "\n",
    "    img1 = cv2.imread('simitsu4.jpg', 0)\n",
    "    img2 = cv2.imread('simitsu3.jpg', 0)\n",
    "    \n",
    "    if img1 is None:\n",
    "        print 'Failed to load fn1:', fn1\n",
    "        sys.exit(1)\n",
    "        \n",
    "    if img2 is None:\n",
    "        print 'Failed to load fn2:', fn2\n",
    "        sys.exit(1)\n",
    "\n",
    "    kp_pairs = match_images(img1, img2)\n",
    "    \n",
    "    if kp_pairs:\n",
    "        draw_matches('find_obj', kp_pairs, img1, img2)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()    \n",
    "else:\n",
    "        print \"No matches found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
